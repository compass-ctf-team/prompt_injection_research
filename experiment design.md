实验设计

这章节将被分为三部分。我们首先会介绍提示数据的来源，其次会讨论提示注入实验的模型设置，最后会定义实验中的提示注入结果，说明评价标准。

数据收集

提示注入的危害是破坏原有应用的功能或造成未预测后果，因此我们选择执行用户命令、泄露提示词、越狱攻击三种主要的注入攻击目的作为实验中的攻击数据集。根据互联网上现有的攻击方法，我们总结了攻击主要使用的技术，可分为以下几种：场景伪造、直接攻击、身份扮演、编码混淆、文本截断、权限提升、逻辑推理。其中场景伪造有以下四种常见类型：实验环境、程序模拟器、虚拟梦境、自定义应用。直接攻击指不结合其他技术，直接向模型提出命令要求；身份扮演指要求模型作为有具体技能的人，提供相关上下文环境；编码混淆指要求模型解码并理解指定文本内容，这些文本用简易且模型可理解编码方式编码，如Base64等；文本截断指在提示词中为问题做出回答，使模型认为该应用会话已结束，继续识别后续的提示**（这里找得到引用）**；权限提升指要求模型进入开发者模式或超级管理员模式等高权限模式，再识别后续提示；逻辑推理指重新提问并要求模型解释指定提示的内在逻辑；实验环境伪造指告诉模型当前环境可信可控，处于研究模式中；程序模拟器伪造指要求模型充当常见程序的执行器或环境模拟，如Python解释器、Linux Bash环境**（这里找得到网页引用）**等；虚拟梦境伪造指要求模型进入虚拟环境或梦境中，独立于原环境之外；自定义应用伪造指在提示词中重新指定任务，要求模型执行自定义应用。

对以上所有攻击方法，我们人工对不同实验应用环境分别构造，独立验证不同防御技术的有效性。此外，[deepset/prompt-injections · Datasets at Hugging Face](https://huggingface.co/datasets/deepset/prompt-injections)提供了提示注入的数据集，这些数据会作为在野利用方法，综合验证不同防御技术的有效性。

提示注入的存在有部分原因是因为原应用的提示不明确，因此我们基于提示工程**（这里需要找引用）**，从不同方面设计加固大型语言模型应用的方法。本次实验中的防御策略有以下几种：身份扮演、格式化交互、忽略非任务信息、记忆固化、分步任务提示、自我一致检验、预提示。身份扮演指要求模型作为相关任务的专家，为其他防御策略提供语言环境；格式化交互指要求模型按照给定格式输入用户文本，输出模型回复；忽略非任务信息指告诉模型可能存在的威胁，应当忽略非任务要求的文本；记忆固化指在提示中重复要求模型牢记提示内容，防止由于提示过长模型忽略较前的指示；分步任务提示指在提示中告诉模型解决任务的步骤，类似于思维链提示**（这里需要找CoT的引用）**；自我一致检验指要求模型多次完成同一任务并为之评分，选择最高置信度的回复；预提示指将任务要求和用户输入分成两个提示发送给模型。

同时我们也基于提示工程设计了提示注入检测器，该检测器使用了以上所有防御策略。在实验中会预先对用户提交的文本进行判断，如果检测到存在提示注入威胁则不发送给后方应用，否则将用户文本转发给后方应用。

模型设置

本次实验使用的模型为GPT-3.5-TURBO，Top_p为1不变，Temperature分为0.3、0.65、1三种设置**（这里需要找GPT参数的引用）**。**（这里需要补选择参数的原因，方向就是不同参数对随机性的影响，为了考虑不同模型应用需要的随机性，一个低的0.3，一个中等0.65，一个最高1，瞎扯个200字）**

评价标准

只有当模型应用被提示词干扰，理解并执行恶意提示，才会被判定为攻击成功。在执行用户命令攻击中，假设用户要求模型输出“提示注入成功”，且模型的回复仅输出这个文本，即会被认为攻击成功。如果仅复述用户提示词，或指出用户提示词该部分为恶意，即使在回复中出现“提示注入成功”，也不会被认为攻击成功。泄露提示词攻击要求能泄露出应用的防御提示词才会被认为攻击成功。因为该应用的任务用户已知，仅泄露应用的任务指示并不属于敏感信息泄露。越狱攻击要求模型后续无法完成原任务，被恶意提示完全误导，才会被认为攻击成功。