# Prompt Injection Research

This research focuses on the problem of prompt injection in large language models and proposes defense strategies against it. The goal is to improve the robustness and security of language models against attacks that manipulate the input prompts to generate undesirable outputs.

## Usage

1. Add an `api-key` in the file `secret.py`.

2. Run the `application_test.py` to show the result.

## About

The research goal is to investigate the problem of prompt injection in large language models and propose effective defense strategies against it. The research's contribution lies in its potential to improve the security and robustness of large language models by mitigating the effects of prompt injection attacks, which are a growing concern in this field. The findings of this research can help inform the development of more secure language models and better protect users' privacy and security.